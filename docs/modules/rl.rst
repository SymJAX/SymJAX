.. _symjax-rl:

:mod:`symjax.rl`
---------------------------



**Notions**

    - immediate reward :math:`r_t` is observed from the environment at state :math:`ğ‘ _{t}` by performing action :math:`ğ‘_{t}`

    - total discounted reward :math:`ğº_t(Î³)` often abbreviated as :math:`ğº_t` and defined as

      .. math::
          ğº_t = Î£_{t'=t+1}^{T}Î³^{t'-t-1}r_t

    - action-value function :math:`Q_{Ï€}(ğ‘ ,ğ‘)` is the expected return starting from state ğ‘ , following policy ğœ‹ and taking action ğ‘

      .. math::
          Q_{Ï€}(ğ‘ ,ğ‘)=E_{Ï€}[ğº_{t}|ğ‘ _{t} = ğ‘ ,ğ‘_{t}=ğ‘]

    - state-value function :math:`V_{Ï€}(ğ‘ )` is the expected return starting from state ğ‘  following policy ğœ‹ as in

      .. math::
          V_{Ï€}(ğ‘ )&=E_{Ï€}[ğº_{t}|ğ‘ _{t} = ğ‘ ]\\
                &=Î£_{ğ‘ âˆˆ ğ´}Ï€(ğ‘|ğ‘ )Q_{Ï€}(ğ‘ ,ğ‘)

      in a deterministic policy setting, one has directly :math:`V_{Ï€}(ğ‘ )=Q_{Ï€}(ğ‘ ,Ï€(ğ‘ ))`.
      in a greedy policy one might have :math:`V^{*}_{Ï€}(ğ‘ )=\max_{ğ‘âˆˆğ´}Q_{Ï€}(ğ‘ ,ğ‘)` where :math:`V^{*}_{Ï€}` is the best value of a state if you could follow an (unknown) optimum policy.

    - TD-error

        + :math:`ğ›¿_t=r_t+Î³Q(ğ‘ _{t+1},ğ‘_{t+1})-Q(ğ‘ _{t},ğ‘_{t})`

    - advantage value : how much better it is to take a specific action compared to the average at the given state

      .. math::
          A(s_t,ğ‘_t)&=Q(ğ‘ _t,ğ‘_t)-V(ğ‘ _t)\\
          A(ğ‘ _t,ğ‘_t)&=E[r_{t+1}+ Î³ V(ğ‘ _{t+1})]-V(ğ‘ _t)\\
          A(ğ‘ _t,ğ‘_t)&=r_{t+1}+ Î³ V(ğ‘ _{t+1})-V(ğ‘ _t)

      The formulation of policy gradients with advantage functions is extremely common, and there are `many different ways <https://arxiv.org/abs/1506.02438>`_ of estimating the advantage function used by different algorithms.

    - probability of a trajectory :math:`Ï„=(s_0,a_0,...,s_{T+1})` is given by

      .. math::
          p(Ï„|Î¸)=p_{0}(s_0)Î _{t=0}^{T}p(ğ‘ _{t+1}|ğ‘ _{t},ğ‘_{t})Ï€_{0}(ğ‘_{t}|ğ‘ _{t})


**Models**

    - `Policy gradient and REINFORCE <https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63>`_ : Policy gradient methods are ubiquitous in model free reinforcement learning algorithms â€” they appear frequently in reinforcement learning algorithms, especially so in recent publications. The policy gradient method is also the â€œactorâ€ part of Actor-Critic methods. Its implementation (REINFORCE) is also known as Monte Carlo Policy Gradients. Policy gradient methods update the probability distribution of actions :math:`Ï€(a|s)` so that actions with higher expected reward have a higher probability value for an observed state.

        + needs to reach end of episode to compute discounted rewards and train the model
        + only needs an actor (a.k.a policy) network
        + noisy gradients and high variance => instability and slow convergence
        + fails for trajectories having a cumulative reward of 0

**Tricks**

    - `normalizing discounter rewards (or advantages) <http://arxiv.org/abs/1506.02438>`_ : In practice it can can also be important to normalize these. For example, suppose we compute [discounted cumulative reward] for all of the 20,000 actions in the batch of 100 Pong game rollouts above. One good idea is to â€œstandardizeâ€ these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way weâ€™re always encouraging and discouraging roughly half of the performed actions. Mathematically you can also interpret these tricks as a way of controlling the variance of the policy gradient estimator.


Implementation of basic agents, environment utilites and learning policies

.. automodule:: symjax.rl.utils


..  autosummary::

	Buffer
	run
	

.. automodule:: symjax.rl.agents


..  autosummary::

	Actor
	Critic


.. automodule:: symjax.rl

..  autosummary::
	REINFORCE
	ActorCritic
	PPO
	DDPG





Detailed Descriptions
=====================

.. automodule:: symjax.rl.utils

.. autoclass:: Buffer
.. autofunction:: run

.. automodule:: symjax.rl.agents

.. autoclass:: Actor
.. autoclass:: Critic

.. automodule:: symjax.rl

.. autoclass:: REINFORCE
.. autoclass:: ActorCritic
.. autoclass:: PPO
.. autoclass:: DDPG